#!/usr/bin/env python3
"""
é«˜å¹¶å‘æ²™ç®±æ‰§è¡Œç¯å¢ƒå»¶è¿Ÿæµ‹è¯•è„šæœ¬

è¯¥è„šæœ¬ç”¨äºæµ‹é‡ code-judge æ²™ç®±åœ¨ä¸åŒå¹¶å‘çº§åˆ«ä¸‹çš„å¹³å‡å»¶è¿Ÿæƒ…å†µï¼Œ
å¸®åŠ©è¯†åˆ«æ€§èƒ½ç“¶é¢ˆå’Œé˜»å¡ç‚¹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python benchmark_latency.py --host http://localhost:8000 --max-concurrency 100
"""

import argparse
import asyncio
import aiohttp
import time
import statistics
from dataclasses import dataclass, field
from typing import List, Dict
import json
from collections import defaultdict
import sys


@dataclass
class LatencyStats:
    """å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯"""
    concurrency: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_latency: float
    median_latency: float
    p95_latency: float
    p99_latency: float
    min_latency: float
    max_latency: float
    requests_per_second: float
    total_duration: float
    
    def __str__(self):
        return (
            f"{self.concurrency:<10} "
            f"{self.total_requests:<12} "
            f"{self.successful_requests}/{self.total_requests:<15} "
            f"{self.avg_latency:>7.2f}ms    "
            f"{self.median_latency:>7.2f}ms    "
            f"{self.p95_latency:>7.2f}ms    "
            f"{self.p99_latency:>7.2f}ms    "
            f"{self.requests_per_second:>6.1f}"
        )


@dataclass
class BenchmarkConfig:
    """å‹åŠ›æµ‹è¯•é…ç½®"""
    host: str = "http://localhost:8000"
    endpoint: str = "/judge"
    min_concurrency: int = 1
    max_concurrency: int = 100
    concurrency_step: int = 5
    batches_per_level: int = 10  # æ¯ä¸ªå¹¶å‘çº§åˆ«æ‰§è¡Œçš„æ‰¹æ¬¡æ•°ï¼ˆæ€»è¯·æ±‚æ•° = å¹¶å‘æ•° Ã— æ‰¹æ¬¡æ•°ï¼‰
    timeout: int = 30
    warmup_requests: int = 10
    
    # æµ‹è¯•ç”¨ä¾‹é…ç½®
    test_cases: List[Dict] = field(default_factory=lambda: [
        # è½»é‡çº§æµ‹è¯•
        {
            "type": "python",
            "solution": "print(input())",
            "input": "hello",
            "expected_output": "hello"
        },
        # ä¸­ç­‰è®¡ç®—é‡ - 0.5ç§’å»¶è¿Ÿ + è®¡ç®—
        {
            "type": "python",
            "solution": "import time\ntime.sleep(0.5)\nprint(sum([int(x) for x in input().split()]))",
            "input": "1 2 3 4 5",
            "expected_output": "15"
        },
        # è¾ƒé‡çš„è®¡ç®— - 1ç§’å»¶è¿Ÿ + å¤æ‚è®¡ç®—
        {
            "type": "python",
            "solution": """import time
time.sleep(1.0)
n = int(input())
result = sum(i*i for i in range(n))
print(result)""",
            "input": "1000",
            "expected_output": "332833500"
        },
        # CPUå¯†é›†å‹ - æ–æ³¢é‚£å¥‘è®¡ç®—
        {
            "type": "python",
            "solution": """def fib(n):
    if n <= 1: return n
    a, b = 0, 1
    for _ in range(2, n+1):
        a, b = b, a + b
    return b
n = int(input())
print(fib(n))""",
            "input": "10000",
            "expected_output": "33644764876431783266621612005107543310302148460680063906564769974680081442166662368155595513633734025582065332680836159373734790483865268263040892463056431887354544369559827491606602099884183933864652731300088830269235673613135117579297437854413752130520504347701602264758318906527890855154366159582987279682987510631200575428783453215515103870818298969791613127856265033195487140214287532698187962046936097879900350962302291026368131493195275630227837628441540360584402572114334961180023091208287046088923962328835461505776583271252546093591128203925285393434620904245248929403901706233888991085841065183173360437470737908552631764325733993712871937587746897479926305837065742830161637408969178426378624212835258112820516370298089332099905707920064367426202389783111470054074998459250360633560933883831923386783056136435351892133279732908133732642652633989763922723407882928177953580570993691049175470808931841056146322338217465637321248226383092103297701648054726243842374862411453093812206564914032751086643394517512161526545361333111314042436854805106765843493523836959653428071768775328348234345557366719731392746273629108210679280784718035329131176778924659089938635459327894523777674406192240337638674004021330343297496902028328145933418826817683893072003634795623117103101291953169794607632737589253530772552375943788434504067715555779056450443016640119462580972216729758615026968443146952034614932291105970676243268515992834709891284706740862008587135016260312071903172086094081298321581077282076353186624611278245537208532365305775956430072517744315051539600905168603220349163222640885248852433158051534849622434848299380905070483482449327453732624567755879089187190803662058009594743150052402532709746995318770724376825907419939632265984147498193609285223945039707165443156421328157688908058783183404917434556270520223564846495196112460268313970975069382648706613264507665074611512677522748621598642530711298441182622661057163515069260029861704945425047491378115154139941550671256271197133252763631939606902895650288268608362241082050562430701794976171121233066073310059947366875"
        },
        # IOå¯†é›†å‹ - æ–‡ä»¶æ“ä½œæ¨¡æ‹Ÿ
        {
            "type": "python",
            "solution": """import time
# æ¨¡æ‹ŸIOæ“ä½œ
time.sleep(0.3)
data = input().split(',')
result = ','.join(sorted(data))
print(result)""",
            "input": "z,a,m,b,x,c",
            "expected_output": "a,b,c,m,x,z"
        },
    ])


class LatencyBenchmark:
    """å»¶è¿Ÿå‹åŠ›æµ‹è¯•ç±»"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.results: List[LatencyStats] = []
        
    async def send_request(self, session: aiohttp.ClientSession, test_case: Dict) -> tuple[float, bool]:
        """
        å‘é€å•ä¸ªè¯·æ±‚å¹¶æµ‹é‡å»¶è¿Ÿ
        
        è¿”å›: (å»¶è¿Ÿ(æ¯«ç§’), æ˜¯å¦æˆåŠŸ)
        """
        url = f"{self.config.host}{self.config.endpoint}"
        start_time = time.perf_counter()
        
        try:
            async with session.post(
                url,
                json=test_case,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            ) as response:
                await response.read()
                latency = (time.perf_counter() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                success = response.status == 200
                return latency, success
        except asyncio.TimeoutError:
            latency = (time.perf_counter() - start_time) * 1000
            print(f"âš ï¸  è¯·æ±‚è¶…æ—¶ (>{self.config.timeout}s)", file=sys.stderr)
            return latency, False
        except Exception as e:
            latency = (time.perf_counter() - start_time) * 1000
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
            return latency, False
    
    async def run_concurrent_requests(self, concurrency: int, num_requests: int) -> List[tuple[float, bool]]:
        """
        è¿è¡ŒæŒ‡å®šå¹¶å‘çº§åˆ«çš„è¯·æ±‚
        
        Args:
            concurrency: å¹¶å‘æ•°
            num_requests: æ€»è¯·æ±‚æ•°
            
        Returns:
            åŒ…å«æ‰€æœ‰è¯·æ±‚ç»“æœçš„åˆ—è¡¨: [(å»¶è¿Ÿ, æ˜¯å¦æˆåŠŸ), ...]
        """
        results = []
        
        # åˆ›å»ºè¿æ¥æ± 
        connector = aiohttp.TCPConnector(limit=concurrency, limit_per_host=concurrency)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
            tasks = []
            test_case_index = 0
            
            for i in range(num_requests):
                test_case = self.config.test_cases[test_case_index % len(self.config.test_cases)]
                task = self.send_request(session, test_case)
                tasks.append(task)
                test_case_index += 1
                
                # æ§åˆ¶å¹¶å‘æ•°
                if len(tasks) >= concurrency:
                    batch_results = await asyncio.gather(*tasks)
                    results.extend(batch_results)
                    tasks = []
            
            # å¤„ç†å‰©ä½™ä»»åŠ¡
            if tasks:
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)
        
        return results
    
    def calculate_stats(self, concurrency: int, results: List[tuple[float, bool]], duration: float) -> LatencyStats:
        """
        è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            concurrency: å¹¶å‘æ•°
            results: è¯·æ±‚ç»“æœåˆ—è¡¨
            duration: æ€»è€—æ—¶(ç§’)
            
        Returns:
            LatencyStats å¯¹è±¡
        """
        latencies = [lat for lat, _ in results]
        successes = [success for _, success in results]
        
        successful_requests = sum(successes)
        failed_requests = len(successes) - successful_requests
        
        if not latencies:
            return LatencyStats(
                concurrency=concurrency,
                total_requests=0,
                successful_requests=0,
                failed_requests=0,
                avg_latency=0,
                median_latency=0,
                p95_latency=0,
                p99_latency=0,
                min_latency=0,
                max_latency=0,
                requests_per_second=0,
                total_duration=duration
            )
        
        sorted_latencies = sorted(latencies)
        
        return LatencyStats(
            concurrency=concurrency,
            total_requests=len(results),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            avg_latency=statistics.mean(latencies),
            median_latency=statistics.median(latencies),
            p95_latency=sorted_latencies[int(len(sorted_latencies) * 0.95)],
            p99_latency=sorted_latencies[int(len(sorted_latencies) * 0.99)],
            min_latency=min(latencies),
            max_latency=max(latencies),
            requests_per_second=len(results) / duration if duration > 0 else 0,
            total_duration=duration
        )
    
    async def warmup(self):
        """é¢„çƒ­æœåŠ¡"""
        print(f"ğŸ”¥ é¢„çƒ­ä¸­... (å‘é€ {self.config.warmup_requests} ä¸ªè¯·æ±‚)")
        connector = aiohttp.TCPConnector(limit=5)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = []
            for i in range(self.config.warmup_requests):
                test_case = self.config.test_cases[i % len(self.config.test_cases)]
                tasks.append(self.send_request(session, test_case))
            await asyncio.gather(*tasks)
        print("âœ… é¢„çƒ­å®Œæˆ\n")
    
    async def run_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„å‹åŠ›æµ‹è¯•"""
        print("=" * 120)
        print("ğŸš€ å¼€å§‹é«˜å¹¶å‘å»¶è¿Ÿå‹åŠ›æµ‹è¯•")
        print(f"ğŸ“ ç›®æ ‡æœåŠ¡: {self.config.host}{self.config.endpoint}")
        print(f"ğŸ“Š æµ‹è¯•èŒƒå›´: å¹¶å‘æ•° {self.config.min_concurrency} ~ {self.config.max_concurrency} (æ­¥é•¿: {self.config.concurrency_step})")
        print(f"ğŸ“¦ æ¯çº§æ‰¹æ¬¡æ•°: {self.config.batches_per_level} (æ€»è¯·æ±‚æ•° = å¹¶å‘æ•° Ã— {self.config.batches_per_level})")
        print("=" * 120)
        print()
        
        # é¢„çƒ­
        await self.warmup()
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = range(
            self.config.min_concurrency,
            self.config.max_concurrency + 1,
            self.config.concurrency_step
        )
        
        print(f"{'å¹¶å‘æ•°':<10} {'æ€»è¯·æ±‚':<12} {'æˆåŠŸ/æ€»æ•°':<15} {'å¹³å‡å»¶è¿Ÿ':<12} {'ä¸­ä½æ•°':<12} {'P95':<12} {'P99':<12} {'QPS':<10}")
        print("-" * 120)
        
        for concurrency in concurrency_levels:
            # è®¡ç®—è¯¥å¹¶å‘çº§åˆ«çš„æ€»è¯·æ±‚æ•°ï¼ˆå¹¶å‘æ•° Ã— æ‰¹æ¬¡æ•°ï¼‰
            num_requests = concurrency * self.config.batches_per_level
            
            start_time = time.perf_counter()
            results = await self.run_concurrent_requests(concurrency, num_requests)
            duration = time.perf_counter() - start_time
            
            stats = self.calculate_stats(concurrency, results, duration)
            self.results.append(stats)
            
            print(stats)
            
            # å¦‚æœå¤±è´¥ç‡è¿‡é«˜ï¼Œåœæ­¢æµ‹è¯•
            if stats.failed_requests / stats.total_requests > 0.5:
                print(f"\nâš ï¸  è­¦å‘Š: å¤±è´¥ç‡è¶…è¿‡50%ï¼Œåœæ­¢æµ‹è¯•")
                break
            
            # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿‡åº¦å‹åŠ›
            await asyncio.sleep(0.5)
        
        print("\n" + "=" * 120)
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\nğŸ“Š æµ‹è¯•æ‘˜è¦")
        print("=" * 120)
        
        if not self.results:
            print("æ²¡æœ‰æ”¶é›†åˆ°æµ‹è¯•æ•°æ®")
            return
        
        # æ‰¾å‡ºæ€§èƒ½æ‹ç‚¹
        print("\nğŸ” æ€§èƒ½åˆ†æ:")
        
        baseline_latency = self.results[0].avg_latency
        significant_degradation_found = False
        
        for i, stats in enumerate(self.results):
            if i == 0:
                continue
            
            latency_increase = (stats.avg_latency - baseline_latency) / baseline_latency * 100
            
            # å¦‚æœå»¶è¿Ÿå¢åŠ è¶…è¿‡50%ï¼Œè®¤ä¸ºå‡ºç°æ˜¾è‘—é˜»å¡
            if latency_increase > 50 and not significant_degradation_found:
                print(f"\nâš ï¸  æ˜¾è‘—æ€§èƒ½ä¸‹é™ç‚¹: å¹¶å‘æ•° {stats.concurrency}")
                print(f"   - å¹³å‡å»¶è¿Ÿä» {baseline_latency:.2f}ms å¢åŠ åˆ° {stats.avg_latency:.2f}ms (+{latency_increase:.1f}%)")
                print(f"   - P99å»¶è¿Ÿ: {stats.p99_latency:.2f}ms")
                print(f"   - å»ºè®®æœ€å¤§å¹¶å‘æ•°: {self.results[i-1].concurrency}")
                significant_degradation_found = True
        
        if not significant_degradation_found:
            best_qps_stats = max(self.results, key=lambda s: s.requests_per_second)
            print(f"\nâœ… åœ¨æµ‹è¯•èŒƒå›´å†…æœªå‘ç°æ˜¾è‘—é˜»å¡")
            print(f"   - æœ€ä½³æ€§èƒ½ç‚¹: å¹¶å‘æ•° {best_qps_stats.concurrency}, QPS: {best_qps_stats.requests_per_second:.1f}")
        
        # æœ€ä½³æ€§èƒ½ç‚¹
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        best_qps = max(self.results, key=lambda s: s.requests_per_second)
        print(f"   - æœ€é«˜QPS: {best_qps.requests_per_second:.1f} (å¹¶å‘æ•°: {best_qps.concurrency})")
        
        lowest_latency = min(self.results, key=lambda s: s.avg_latency)
        print(f"   - æœ€ä½å¹³å‡å»¶è¿Ÿ: {lowest_latency.avg_latency:.2f}ms (å¹¶å‘æ•°: {lowest_latency.concurrency})")
        
        highest_latency = max(self.results, key=lambda s: s.avg_latency)
        print(f"   - æœ€é«˜å¹³å‡å»¶è¿Ÿ: {highest_latency.avg_latency:.2f}ms (å¹¶å‘æ•°: {highest_latency.concurrency})")
        
        print("\n" + "=" * 120)
    
    def export_results(self, filename: str = "latency_benchmark_results.json"):
        """å¯¼å‡ºç»“æœåˆ°JSONæ–‡ä»¶"""
        data = {
            "config": {
                "host": self.config.host,
                "min_concurrency": self.config.min_concurrency,
                "max_concurrency": self.config.max_concurrency,
                "batches_per_level": self.config.batches_per_level,
            },
            "results": [
                {
                    "concurrency": s.concurrency,
                    "total_requests": s.total_requests,
                    "successful_requests": s.successful_requests,
                    "failed_requests": s.failed_requests,
                    "avg_latency_ms": s.avg_latency,
                    "median_latency_ms": s.median_latency,
                    "p95_latency_ms": s.p95_latency,
                    "p99_latency_ms": s.p99_latency,
                    "min_latency_ms": s.min_latency,
                    "max_latency_ms": s.max_latency,
                    "requests_per_second": s.requests_per_second,
                    "total_duration_s": s.total_duration,
                }
                for s in self.results
            ]
        }
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")


def main():
    parser = argparse.ArgumentParser(
        description="é«˜å¹¶å‘æ²™ç®±æ‰§è¡Œç¯å¢ƒå»¶è¿Ÿæµ‹è¯•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬æµ‹è¯• (å¹¶å‘1-100ï¼Œæ­¥é•¿5ï¼Œæ¯çº§æ‰¹æ¬¡æ•°10)
  # ä¾‹å¦‚å¹¶å‘10æ—¶å‘é€10Ã—10=100ä¸ªè¯·æ±‚ï¼Œå¹¶å‘50æ—¶å‘é€50Ã—10=500ä¸ªè¯·æ±‚
  python benchmark_latency.py
  
  # è‡ªå®šä¹‰æµ‹è¯•èŒƒå›´å’Œæ‰¹æ¬¡æ•°
  python benchmark_latency.py --min-concurrency 10 --max-concurrency 200 --step 10 --batches-per-level 20
  
  # å¢åŠ æ‰¹æ¬¡æ•°ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœï¼ˆä½†ä¼šå¢åŠ æµ‹è¯•æ—¶é—´ï¼‰
  python benchmark_latency.py --batches-per-level 50
  
  # æµ‹è¯•è¿œç¨‹æœåŠ¡å™¨
  python benchmark_latency.py --host http://192.168.1.100:8000
  
æ³¨æ„: æ€»è¯·æ±‚æ•° = å¹¶å‘æ•° Ã— æ‰¹æ¬¡æ•°
      ä¾‹å¦‚: å¹¶å‘50ï¼Œæ‰¹æ¬¡10ï¼Œæ€»å…±ä¼šå‘é€ 50Ã—10=500 ä¸ªè¯·æ±‚
        """
    )
    
    parser.add_argument(
        "--host",
        default="http://localhost:8088",
        help="judge æœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:8088)"
    )
    
    parser.add_argument(
        "--endpoint",
        default="/judge",
        help="APIç«¯ç‚¹ (é»˜è®¤: /judge)"
    )
    
    parser.add_argument(
        "--min-concurrency",
        type=int,
        default=1,
        help="æœ€å°å¹¶å‘æ•° (é»˜è®¤: 1)"
    )
    
    parser.add_argument(
        "--max-concurrency",
        type=int,
        default=100,
        help="æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 100)"
    )
    
    parser.add_argument(
        "--step",
        type=int,
        default=5,
        help="å¹¶å‘æ•°æ­¥é•¿ (é»˜è®¤: 5)"
    )
    
    parser.add_argument(
        "--batches-per-level",
        type=int,
        default=10,
        help="æ¯ä¸ªå¹¶å‘çº§åˆ«çš„æ‰¹æ¬¡æ•°ï¼Œæ€»è¯·æ±‚æ•°=å¹¶å‘æ•°Ã—æ‰¹æ¬¡æ•° (é»˜è®¤: 10)"
    )
    
    parser.add_argument(
        "--timeout",
        type=int,
        default=30,
        help="å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 30)"
    )
    
    parser.add_argument(
        "--warmup",
        type=int,
        default=10,
        help="é¢„çƒ­è¯·æ±‚æ•° (é»˜è®¤: 10)"
    )
    
    parser.add_argument(
        "--output",
        default="latency_benchmark_results.json",
        help="ç»“æœè¾“å‡ºæ–‡ä»¶ (é»˜è®¤: latency_benchmark_results.json)"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = BenchmarkConfig(
        host=args.host,
        endpoint=args.endpoint,
        min_concurrency=args.min_concurrency,
        max_concurrency=args.max_concurrency,
        concurrency_step=args.step,
        batches_per_level=args.batches_per_level,
        timeout=args.timeout,
        warmup_requests=args.warmup,
    )
    
    # è¿è¡Œæµ‹è¯•
    benchmark = LatencyBenchmark(config)
    
    try:
        asyncio.run(benchmark.run_benchmark())
        benchmark.export_results(args.output)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        if benchmark.results:
            benchmark.print_summary()
            benchmark.export_results(args.output)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
